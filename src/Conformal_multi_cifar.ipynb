{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "798a35ae-229e-49dd-95d2-8960a09ed4df",
   "metadata": {},
   "source": [
    "## (Trying to be) Simple demonstration of Multiview Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21050820",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' In the context of this project:\n",
    "This is an outdated (trying to be simple) demo of what is now the RT4U algorithm.\n",
    "However, the evaluation functions and neural architecture are lacking.\n",
    "- M'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f5b4d-fa70-4757-830d-267dbd933a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9cab2-d6a8-4cfe-baf7-547b49620d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check GPU environment\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_of_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {num_of_gpus}\")\n",
    "    #for i in range(num_of_gpus):\n",
    "    #    freemem, _ = torch.cuda.mem_get_info(i)\n",
    "    #    print(f\"GPU {i}, free memory {freemem/1024} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1afdf4-5ba1-42a0-85c1-829f40aac598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your device manually\n",
    "device = \"cuda:2\"\n",
    "freemem, _ = torch.cuda.mem_get_info(device)\n",
    "print(f\"Using device: {device}, free memory {freemem/1024/1024} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbe7e5-dd96-421e-86e6-b62830215e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract quadrant from PIL image\n",
    "\n",
    "def index_to_quadrant_id(index):\n",
    "    # works w/ integers and numpy arrays\n",
    "    img_index = index // 4\n",
    "    quadrant_id = index % 4\n",
    "    return img_index, quadrant_id\n",
    "\n",
    "def get_quadrant(img, quadrant_id):\n",
    "    # PIL image -> PIL image\n",
    "    w, h = img.size\n",
    "    match quadrant_id:\n",
    "    # quadrant ID is 0:top_left, 1:top_right, 2:bottom_left, 3:bottom_right\n",
    "        case 0:\n",
    "            window = (0, 0, w//2, h//2)\n",
    "        case 1:\n",
    "            window = (w//2, 0, w, h//2)\n",
    "        case 2:\n",
    "            window = (0, h//2, w//2, h)\n",
    "        case 3:\n",
    "            window = (w//2, h//2, w, h)\n",
    "        case _:\n",
    "            print(\"Invalid quadrant ID\")\n",
    "    img = img.crop(window)\n",
    "    img = img.resize((w, h))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba77dcf-9da4-4962-8860-b429d5e6ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to define a new dataset for taking out image quadrants from CIFAR\n",
    "# to do this, we will write a superclass for CIFAR dataset\n",
    "\n",
    "class CIFAR10_Quadrants(torchvision.datasets.CIFAR10):\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Accesses a quadrant of an image in CIFAR\n",
    "        Args:\n",
    "            index (int): quadrant-based index, which is equal to 4*image_index + quadrant ID\n",
    "            quadrant ID is 0:top_left, 1:top_right, 2:bottom_left, 3:bottom_right\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img_index, quadrant_id = index_to_quadrant_id(index)\n",
    "        img, target = self.data[img_index], int(self.targets[img_index])\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "        img = get_quadrant(img, quadrant_id)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2629ab-0a74-44e1-a17b-54ae2a70f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CIFAR data into our custom dataset\n",
    "\n",
    "data_root_dir = \"/workspace/datasets/\"\n",
    "if_download = True # set this to false if you don't want to accidentally write anything to disk\n",
    "\n",
    "input_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "data_tr = CIFAR10_Quadrants(root=data_root_dir, train=True, transform=input_transform, download=if_download)\n",
    "data_te = CIFAR10_Quadrants(root=data_root_dir, train=False, transform=input_transform, download=if_download)\n",
    "print(f\"CIFAR10: # training quadrants = {len(data_tr)}, # test quadrants = {len(data_te)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729822bf-5bf9-4341-be27-7877f0910883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yield the following\n",
    "# training dataloader using quadrants from 90% of the examples\n",
    "# calibration dataloader using quadrants from 10% of the examples\n",
    "# test dataloader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "n_calibration_set = len(data_tr) // 10\n",
    "assert n_calibration_set % 4 == 0\n",
    "\n",
    "data_tr_subset = torch.utils.data.Subset(data_tr, np.arange(0, len(data_tr)-n_calibration_set))\n",
    "data_va_subset = torch.utils.data.Subset(data_tr, np.arange(len(data_tr)-n_calibration_set, len(data_tr)))\n",
    "loader_tr = torch.utils.data.DataLoader(data_tr_subset, batch_size=batch_size, shuffle=True)\n",
    "loader_va = torch.utils.data.DataLoader(data_va_subset, batch_size=batch_size, shuffle=True)\n",
    "loader_te = torch.utils.data.DataLoader(data_te, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89673422-1adf-4e5b-97ce-289185bbb332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some images as a sanity check\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "dataiter = iter(loader_va)\n",
    "images, labels, indices = next(dataiter)\n",
    "indices, quads = index_to_quadrant_id(indices)\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print([classes[j] for j in labels])\n",
    "print(indices)\n",
    "print(quads)\n",
    "\n",
    "dataiter = iter(loader_te)\n",
    "images, labels, indices = next(dataiter)\n",
    "indices, quads = index_to_quadrant_id(indices)\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print([classes[j] for j in labels])\n",
    "print(indices)\n",
    "print(quads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b562c45-3f86-44a0-9919-ffde13104222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a small neural network\n",
    "# architecture: LeNet-5 implementation: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101f0b4-2d57-4c15-8b37-99a0bdf63121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a label bank object that stores network predictions during training\n",
    "# the size of the label bank is Nx4xExC, N=tr.set, E=#epochs, C=#classes\n",
    "\n",
    "N_classes = len(classes)\n",
    "N_training_samples = len(data_tr)\n",
    "\n",
    "def evolve_labels(preds_history):\n",
    "    # preds_history is a length-E list of NxC arrays of logits\n",
    "    preds_history_softmax = [softmax(arr, axis=1) for arr in preds_history]\n",
    "    preds_history_softmax = np.array(preds_history_softmax)\n",
    "    augmented_labels = np.mean(preds_history_softmax, axis=0)\n",
    "    return augmented_labels\n",
    "\n",
    "# facilitation of label evolution\n",
    "def label_evolution_loop(loader_tr, loader_va, n_epochs, reset_weights=True, n_evolutions=1):\n",
    "    model = Net()\n",
    "\n",
    "    augmented_labels = None\n",
    "    augmented_labels_tab = []\n",
    "    for ne in range(n_evolutions):\n",
    "        print(f\"Label evolution iteration: {ne}\")\n",
    "        if reset_weights and ne >= 1:\n",
    "            model = Net()\n",
    "            print(f\"Model weights re-initialized\")\n",
    "        model, model_preds_history = train_loop(model, loader_tr, loader_va, n_epochs, augmented_labels)\n",
    "        augmented_labels = evolve_labels(model_preds_history)\n",
    "        augmented_labels_tab.append(augmented_labels)\n",
    "\n",
    "    return model, augmented_labels_tab\n",
    "\n",
    "# main training loop, augmented labels are optional non one-hot labels\n",
    "def train_loop(model, loader_tr, loader_va, n_epochs, augmented_labels=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model_preds_history = []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        model, model_preds_i = train_epoch(model, optimizer, loader_tr, loader_va, augmented_labels, i)\n",
    "        model_preds_history.append(model_preds_i)\n",
    "\n",
    "    return model, model_preds_history\n",
    "\n",
    "def train_epoch(model, optimizer, loader_tr, loader_va, augmented_labels=None, epoch_number=-1):\n",
    "    #model.train() if layer behaviors vary between train/test (eg. dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    # train and record the model outputs\n",
    "    preds = np.zeros((N_training_samples, N_classes))\n",
    "    loss_running = []\n",
    "    acc_running = []\n",
    "    for data in tqdm(loader_tr):\n",
    "        imgs, targets, indices = data\n",
    "        if augmented_labels is None:\n",
    "            targets_gpu = targets.to(device)\n",
    "        else:\n",
    "            targets_gpu = torch.Tensor(augmented_labels[indices]).to(device)\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = F.cross_entropy(outputs, targets_gpu)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        logits = outputs.detach().cpu().numpy() # BxC\n",
    "        for b in range(logits.shape[0]):\n",
    "            preds[indices[b], :] = logits[b, :]\n",
    "        loss_running.append(loss.item())\n",
    "        acc_running.append(np.mean(np.argmax(logits,axis=1) == targets.numpy()))\n",
    "\n",
    "    # validate on the calibration set\n",
    "    loss_running_val = []\n",
    "    acc_running_val = []\n",
    "    for data in tqdm(loader_va):\n",
    "        imgs, targets, indices = data[0].to(device), data[1], data[2]\n",
    "        \n",
    "        outputs = model(imgs)\n",
    "        loss = F.cross_entropy(outputs, targets.to(device))\n",
    "        \n",
    "        logits = outputs.detach().cpu().numpy() # BxC\n",
    "        for b in range(logits.shape[0]):\n",
    "            preds[indices[b], :] = logits[b, :]\n",
    "\n",
    "        loss_running_val.append(loss.item())\n",
    "        acc_running_val.append(np.mean(np.argmax(logits,axis=1) == targets.numpy()))\n",
    "\n",
    "    print(\"Epoch %2d, train loss/acc: %2.3f/%2.3f, val loss/acc: %2.3f/%2.3f\" % \n",
    "          (epoch_number, np.mean(loss_running), np.mean(acc_running), np.mean(loss_running_val), np.mean(acc_running_val)))\n",
    "\n",
    "    return model, preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee17fb-083f-43f5-a445-6e05b2998881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "model, labels_tab = label_evolution_loop(loader_tr, loader_va, n_epochs=40, reset_weights=True, n_evolutions=3)\n",
    "save_path = './cifar_net.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd940d1-3976-46dc-a201-66a39ad6f28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
