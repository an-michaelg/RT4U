{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a993b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' In the context of this project:\n",
    "Contains some of the original code for analysis, however it is outdated.\n",
    "- M'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing with conformal prediction, using the logits tracked from training\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from scipy.special import softmax\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169bfdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the dataframes for training and validation from each epoch\n",
    "# we start by grabbing the dataframes\n",
    "\n",
    "#csv_folder = \"../logs/Baseline_r2plus1d_all/csvs/\"\n",
    "csv_folder = \"../logs/R18_cifar/round1/csvs\"\n",
    "train_csvs = glob.glob(f\"{csv_folder}/train_*.csv\")\n",
    "val_csvs = glob.glob(f\"{csv_folder}/val_*.csv\")\n",
    "\n",
    "N_CLASSES=10\n",
    "ALPHA=0.10\n",
    "\n",
    "def extract_file_index(filename):\n",
    "    # Use regular expression to find numerical part in the filename\n",
    "    match = re.findall(r'\\d+', filename)[-3]\n",
    "    if match:\n",
    "        numerical_part = int(match)\n",
    "        return numerical_part\n",
    "    else:\n",
    "        # Return a default value or handle the case where no numerical part is found\n",
    "        return None\n",
    "\n",
    "# sort ascending by the numerical suffix for each file with regex search\n",
    "def sort_by_epoch(files_list):\n",
    "    indices = []\n",
    "    for i in range(len(files_list)):\n",
    "        index = extract_file_index(files_list[i])\n",
    "        indices.append(index)\n",
    "    new_array = [None] * len(indices)\n",
    "    for j in range(len(indices)):\n",
    "        new_array[indices[j]] = files_list[j]\n",
    "    return new_array\n",
    "\n",
    "print(sort_by_epoch(val_csvs))\n",
    "#print(train_csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a6e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = [pd.read_csv(x) for x in sort_by_epoch(train_csvs)]\n",
    "val_df = [pd.read_csv(x) for x in sort_by_epoch(val_csvs)]\n",
    "\n",
    "# since train_df is ordered randomly, sort each dataframe by the filename\n",
    "train_df = [df.sort_values(by=['filename']).reset_index() for df in train_df]\n",
    "val_df = [df.sort_values(by=['filename']).reset_index() for df in val_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d2727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_logits(df, sm=False):\n",
    "    # take the outputs_x rows and conver them to array of (N, C)\n",
    "    cols = [x for x in df.columns if 'outputs' in x]\n",
    "    logits = df[cols].to_numpy()\n",
    "    if sm:\n",
    "        logits = softmax(logits, axis=1)\n",
    "    return logits\n",
    "\n",
    "train_probs = [df_to_logits(x, False) for x in train_df]\n",
    "val_probs = [df_to_logits(x, False) for x in val_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3382b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the trend-line of prediction F1 score for training and test set\n",
    "def plot_f1_over_time(epoch_0_dataframe, all_probs):\n",
    "    # epoch_0_dataframe is the dataframe results at epoch 0\n",
    "    # probs_progression is the list of probabilities for each epoch\n",
    "    gt = np.array(epoch_0_dataframe['y'])\n",
    "    preds = [np.argmax(p, axis=1) for p in all_probs]\n",
    "    f1s = [f1_score(gt, p, average='macro') for p in preds]\n",
    "    fig, ax = plt.subplots(figsize=(12,4))\n",
    "    t = np.arange(len(preds))\n",
    "    ax.plot(t, f1s)\n",
    "    plt.show()\n",
    "\n",
    "def cf_over_time(epoch_0_dataframe, all_probs):\n",
    "    gt = np.array(epoch_0_dataframe['y'])\n",
    "    preds = [np.argmax(p, axis=1) for p in all_probs]\n",
    "    [print(confusion_matrix(gt, p)) for p in preds]\n",
    "\n",
    "plot_f1_over_time(train_df[0], train_probs)\n",
    "plot_f1_over_time(val_df[0], val_probs)\n",
    "cf_over_time(train_df[0], train_probs)\n",
    "cf_over_time(val_df[0], val_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50079765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are setting up the validation set as the calibration set, to be more kosher, you should use an unseen subset of training\n",
    "# check the certainty distribution of validation set\n",
    "def certainty_hist(y, preds, epoch=-1):\n",
    "    # y is (N,) integer array of C classes\n",
    "    # preds is (N, C) array\n",
    "    pred_classes = np.argmax(preds,axis=1)\n",
    "    pred_confidence = np.max(preds,axis=1)\n",
    "    gt_confidence = preds[range(len(y)),y]\n",
    "    print(f\"Epoch {epoch}, F1 score = {f1_score(y, pred_classes, average='macro')}\")\n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    ax.hist([gt_confidence, pred_confidence], bins=49)\n",
    "    plt.legend(['GT', 'argmax'])\n",
    "    plt.title(f\"Epoch {i} label certainty distribution\")\n",
    "    plt.show()\n",
    "\n",
    "# first check the validation set for naive softmaxing\n",
    "for i in range(len(val_df)):\n",
    "    y = val_df[i]['y']\n",
    "    preds = softmax(val_probs[i], axis=1)\n",
    "    certainty_hist(y, preds, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run conformal prediction on a select epoch to return the prediction sets\n",
    "def LABEL(y, preds, alpha=0.1, verbose=False):\n",
    "    # y is (N,) integer array of C classes\n",
    "    # preds is (N, C) array of confidences\n",
    "    \n",
    "    desired_accuracy = 1 - alpha\n",
    "    N = len(y)\n",
    "    pred_classes = np.argmax(preds,axis=1)\n",
    "    pred_confidence = np.max(preds,axis=1)\n",
    "    gt_confidence = preds[range(N),y]\n",
    "    # calculate the conformal score s as a general imprecise uncertainty measure (can be many ways, this is a simple way)\n",
    "    s = 1 - gt_confidence\n",
    "    # find the q level\n",
    "    adj_q_level = np.ceil((N+1) * desired_accuracy)/N\n",
    "    #adj_q_level = desired_accuracy\n",
    "    qhat = np.quantile(s, adj_q_level)\n",
    "    # create sets based on the q-level\n",
    "    cutoff = 1 - qhat\n",
    "    conformal_set = []\n",
    "    for i in range(N):\n",
    "        conformal_set.append(preds[i] >= cutoff)\n",
    "        \n",
    "    if verbose:\n",
    "        print(f\"adjusted q-level {adj_q_level}, q-hat {qhat}\")\n",
    "    return np.array(conformal_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def APS(cal_labels, cal_smx, alpha=0.1, verbose=False):\n",
    "    # Get scores. calib_X.shape[0] == calib_Y.shape[0] == n\n",
    "    N = cal_smx.shape[0]\n",
    "    \n",
    "    # argsort returns the indices of the sort\n",
    "    cal_pi = cal_smx.argsort(1)[:, ::-1]\n",
    "    \n",
    "    # take_along_axis returns results of the sort, then cumulative sum\n",
    "    cal_srt = np.take_along_axis(cal_smx, cal_pi, axis=1).cumsum(axis=1)\n",
    "    \n",
    "    # this is some 5head stuff I dont get\n",
    "    # but it returns \"the cumulative sum by the time I get to class X\"\n",
    "    cal_scores = np.take_along_axis(cal_srt, cal_pi.argsort(axis=1), axis=1)[range(N), cal_labels]\n",
    "    \n",
    "    # Get the score quantile\n",
    "    adj_q_level = np.ceil((N + 1) * (1 - alpha)) / N\n",
    "    qhat = np.quantile(cal_scores, adj_q_level, interpolation=\"higher\")\n",
    "    if verbose:\n",
    "        print(f\"adjusted q-level {adj_q_level}, q-hat {qhat}\")\n",
    "    \n",
    "    # Deploy (output=list of length n, each element is tensor of classes)\n",
    "    cal_pi = cal_smx.argsort(1)[:, ::-1]\n",
    "    \n",
    "    cal_srt = np.take_along_axis(cal_smx, cal_pi, axis=1).cumsum(axis=1)\n",
    "    \n",
    "    prediction_sets = np.take_along_axis(cal_srt <= qhat, cal_pi.argsort(axis=1), axis=1)\n",
    "    return prediction_sets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e760d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = val_df[2]['y']\n",
    "preds = softmax(val_probs[2], axis=1)\n",
    "cs = APS(y, preds, ALPHA, verbose=True)\n",
    "print(cs[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the coverage of conformal prediction\n",
    "def coverage_test(y, preds_cover):\n",
    "    # y is (N,) integer array of C classes\n",
    "    # preds_cover is (N, C) boolean array indicating the cover\n",
    "    correct = []\n",
    "    for i in range(len(y)):\n",
    "        if preds_cover[i, y[i]]:\n",
    "            correct.append(True)\n",
    "        else:\n",
    "            correct.append(False)\n",
    "    return np.array(correct)\n",
    "\n",
    "correctness = coverage_test(y, cs) \n",
    "print(np.sum(correctness)/len(y)) # should be around 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57875c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_conditional_coverage_test(y, preds_cover, num_classes=4):\n",
    "    # returns an accuracy for each class\n",
    "    class_cond_acc = np.zeros(num_classes)\n",
    "    for c in range(num_classes):\n",
    "        mask = y == c\n",
    "        preds_c = preds_cover[mask]\n",
    "        N_c = len(preds_c)\n",
    "        correct = 0\n",
    "        for i in range(N_c):\n",
    "            if preds_c[i,c]:\n",
    "                correct += 1\n",
    "        class_cond_acc[c] = correct/N_c\n",
    "    return class_cond_acc\n",
    "correctness = class_conditional_coverage_test(y, cs, N_CLASSES) \n",
    "correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1429eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the cardinality of the conformal prediction\n",
    "def cp_cardinality(y, preds_cover, preds, n_classes=4):\n",
    "    cardinality = np.sum(preds_cover, axis=1)\n",
    "    correct = coverage_test(y, preds_cover)\n",
    "    card_correct = cardinality[correct]\n",
    "    card_incorrect = cardinality[~correct]\n",
    "    \n",
    "    n_bins = n_classes + 1\n",
    "    c_amount, c_bins = np.histogram(card_correct, bins=np.arange(n_bins)+0.5)\n",
    "    n_amount, _ = np.histogram(card_incorrect, bins=np.arange(n_bins)+0.5)\n",
    "    card_acc = c_amount / (c_amount + n_amount + 1e-9)\n",
    "    print(f\"Number of correct preds of size [1..N_classes]: {c_amount}\")\n",
    "    print(f\"Number of incorrect preds of size [1..N_classes]: {n_amount}\")\n",
    "    print(f\"Accuracy of preds of size [1..N_classes]: {card_acc}\")\n",
    "\n",
    "    top1_accs = []\n",
    "    pred_classes = np.argmax(preds, axis=1)\n",
    "    for i in np.arange(n_classes)+1:\n",
    "        gt_w_card_i = y[cardinality==i]\n",
    "        pred_w_card_i = pred_classes[cardinality==i]\n",
    "        acc_w_card_i = np.sum(gt_w_card_i == pred_w_card_i) / (len(gt_w_card_i) + 1e-9)\n",
    "        top1_accs.append(acc_w_card_i)\n",
    "    print(f\"Top-1 acc of preds of size [1..N_classes]: {top1_accs}\")\n",
    "    return np.mean(cardinality)\n",
    "\n",
    "cp_cardinality(y, cs, preds, n_classes=N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdf1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run conformal prediction on each epoch\n",
    "f1s = []\n",
    "coverages = []\n",
    "cardinalities = []\n",
    "for i in range(len(val_df)):\n",
    "    y = np.array(val_df[i]['y'])\n",
    "    preds = softmax(val_probs[i], axis=1)\n",
    "    pred_classes = np.argmax(preds,axis=1)\n",
    "    print(f\"Epoch {i}\")\n",
    "    print(f\"Original classifier F1 = {f1_score(y, pred_classes, average='macro')}\")\n",
    "    f1s.append(f1_score(y, pred_classes, average='macro'))\n",
    "    \n",
    "    #cs = LABEL(y, preds, ALPHA)\n",
    "    cs = APS(y, preds, ALPHA)\n",
    "    \n",
    "    correctness = coverage_test(y, cs)\n",
    "    print(f\"Conformal coverage accuracy = {np.sum(correctness)/len(y)}\")\n",
    "    coverages.append(np.sum(correctness)/len(y))\n",
    "    correctness = class_conditional_coverage_test(y, cs, num_classes=N_CLASSES)\n",
    "    print(f\"Class-conditional coverage accuracy = {correctness}\")\n",
    "    avg_card = cp_cardinality(y, cs, preds, n_classes=N_CLASSES)\n",
    "    cardinalities.append(avg_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8286a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(val_df)), f1s)\n",
    "plt.plot(range(len(val_df)), coverages)\n",
    "plt.plot(range(len(val_df)), np.full(shape=len(val_df), fill_value=0.90))\n",
    "plt.title('Label evolution: validation set top-1 F1 vs coverage over training')\n",
    "plt.legend(['top-1 F1 score', 'coverage acc.', 'theoretical coverage acc.'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(val_df)), cardinalities)\n",
    "plt.title('Coverage cardinality over time')\n",
    "plt.legend(['Cardinality'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8c6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
