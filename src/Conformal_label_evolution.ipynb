{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a81dc82-1894-42ad-9755-50b1549ee76d",
   "metadata": {},
   "source": [
    "### Label evolution and conformal prediction as post-processing methods for network predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefbaba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' In the context of this project:\n",
    "This is an old file that experimented with some label evolution+conformal prediction features.\n",
    "The implementation of conformal prediction is not accurate\n",
    "as it does not use the quantile method to find the threshold q_hat.\n",
    "However, it can be seen as a precursor to the project and some of the data visualization code is useful\n",
    "- M'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d909b3-28c2-4806-b80b-22b79e5f8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from scipy.special import softmax\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5399c9-1256-41bc-b2de-f149a16f40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the dataframes for training and validation from each epoch\n",
    "# we start by grabbing the dataframes\n",
    "\n",
    "#csv_folder = \"../logs/Baseline_r2plus1d_all/csvs/\"\n",
    "csv_folder = \"../logs/R2plus1d_all_pseudo_round1/csvs/\"\n",
    "train_csvs = glob.glob(f\"{csv_folder}/train_*.csv\")\n",
    "val_csvs = glob.glob(f\"{csv_folder}/val_*.csv\")\n",
    "data_root = \"/workspace/datasets/Aortic_Stenosis/as_tom/round2/\" # the original CSV which we can depart information from\n",
    "annotations_df = pd.read_csv(os.path.join(data_root, 'annotations-all.csv'))\n",
    "\n",
    "def extract_file_index(filename):\n",
    "    # Use regular expression to find numerical part in the filename\n",
    "    match = re.findall(r'\\d+', filename)[-1]\n",
    "    if match:\n",
    "        numerical_part = int(match)\n",
    "        return numerical_part\n",
    "    else:\n",
    "        # Return a default value or handle the case where no numerical part is found\n",
    "        return None\n",
    "\n",
    "# sort ascending by the numerical suffix for each file with regex search\n",
    "def sort_by_epoch(files_list):\n",
    "    indices = []\n",
    "    for i in range(len(files_list)):\n",
    "        index = extract_file_index(files_list[i])\n",
    "        indices.append(index)\n",
    "    new_array = [None] * len(indices)\n",
    "    for j in range(len(indices)):\n",
    "        new_array[indices[j]] = files_list[j]\n",
    "    return new_array\n",
    "\n",
    "print(sort_by_epoch(train_csvs))\n",
    "\n",
    "train_df = [pd.read_csv(x) for x in sort_by_epoch(train_csvs)]\n",
    "val_df = [pd.read_csv(x) for x in sort_by_epoch(val_csvs)]\n",
    "\n",
    "# since train_df is ordered randomly, sort each dataframe by the filename\n",
    "train_df = [df.sort_values(by=['filename']).reset_index() for df in train_df]\n",
    "val_df = [df.sort_values(by=['filename']).reset_index() for df in val_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15e527-11a6-4ee8-8da2-1d5db93a2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_logits(df, sm=False):\n",
    "    # take the outputs_x rows and conver them to array of (N, C)\n",
    "    cols = [x for x in df.columns if 'outputs' in x]\n",
    "    logits = df[cols].to_numpy()\n",
    "    if sm:\n",
    "        logits = softmax(logits, axis=1)\n",
    "    return logits\n",
    "\n",
    "train_probs = [df_to_logits(x, True) for x in train_df]\n",
    "val_probs = [df_to_logits(x, True) for x in val_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14f262-29a5-4e0c-885d-c410360b9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the trend-line of prediction F1 score for training and test set\n",
    "def plot_f1_over_time(epoch_0_dataframe, all_probs):\n",
    "    # epoch_0_dataframe is the dataframe results at epoch 0\n",
    "    # probs_progression is the list of probabilities for each epoch\n",
    "    gt = np.array(epoch_0_dataframe['y'])\n",
    "    preds = [np.argmax(p, axis=1) for p in all_probs]\n",
    "    f1s = [f1_score(gt, p, average='macro') for p in preds]\n",
    "    fig, ax = plt.subplots(figsize=(12,4))\n",
    "    t = np.arange(len(preds))\n",
    "    ax.plot(t, f1s)\n",
    "    plt.show()\n",
    "\n",
    "def cf_over_time(epoch_0_dataframe, all_probs):\n",
    "    gt = np.array(epoch_0_dataframe['y'])\n",
    "    preds = [np.argmax(p, axis=1) for p in all_probs]\n",
    "    [print(confusion_matrix(gt, p)) for p in preds]\n",
    "\n",
    "plot_f1_over_time(train_df[0], train_probs)\n",
    "plot_f1_over_time(val_df[0], val_probs)\n",
    "cf_over_time(train_df[0], train_probs)\n",
    "cf_over_time(val_df[0], val_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0d23e-c899-466b-95b0-4b92ba72e7f6",
   "metadata": {},
   "source": [
    "### Label evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12846c12-1065-44c5-b7f3-eb8ddae7eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cine_thumbnail(filename):\n",
    "    cine_original = loadmat(filename)[\"cine\"]  # T_original xHxW\n",
    "    T_original = len(cine_original)\n",
    "    thumbnail = cine_original[T_original//2] # HxW\n",
    "    thumbnail = resize(thumbnail, (224,224))  # HxW, Note range becomes [0,1] here\n",
    "    return thumbnail\n",
    "\n",
    "# show the progression of predictions for a specific example\n",
    "def plot_pred_progression(example_id, epoch_0_dataframe, all_probs):\n",
    "    # example id is the row number in the epoch_0_dataframe\n",
    "    # epoch_0_dataframe is the dataframe results at epoch 0\n",
    "    # probs_progression is the list of probabilities for each epoch\n",
    "    correct_class = epoch_0_dataframe.iloc[example_id]['y']\n",
    "    filename = epoch_0_dataframe.iloc[example_id]['filename']\n",
    "    example_probs = np.array([all_probs[t][example_id] for t in range(len(all_probs))]) # TxC\n",
    "    thumbnail = get_cine_thumbnail(filename)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
    "    t = np.arange(len(example_probs))\n",
    "    ax1.imshow(thumbnail, cmap='gray')\n",
    "    ax2.plot(t, example_probs)\n",
    "    plt.suptitle(f\"{filename}, correct class = {correct_class}\")\n",
    "    plt.legend(['0','1','2','3'])\n",
    "    plt.show()\n",
    "\n",
    "def plot_pred_progression_study_level(echo_id, epoch_0_dataframe, all_probs):\n",
    "    # first find a list of images for a certain study\n",
    "    # then plot the training progression for each\n",
    "    matching_entries = annotations_df[annotations_df['Echo ID#']==echo_id]['path']\n",
    "    if len(matching_entries) <= 0:\n",
    "        print(f\"Found 0 matches for {echo_id}, exiting\")\n",
    "        return\n",
    "    print(f\"Found {len(matching_entries)} matches for {echo_id}\")\n",
    "    \n",
    "    for entry in matching_entries:\n",
    "        # find the example id of each matching entry\n",
    "        full_filename = os.path.join(data_root, entry)\n",
    "        entry_index = epoch_0_dataframe[epoch_0_dataframe['filename']==full_filename].index[0]\n",
    "        plot_pred_progression(entry_index, epoch_0_dataframe, all_probs)\n",
    "        \n",
    "print(annotations_df[annotations_df['split']=='train']['Echo ID#'].value_counts())\n",
    "plot_pred_progression_study_level(197635, train_df[0], train_probs)\n",
    "#plot_pred_progression(2, train_df[0], train_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4947cbe-0c01-4018-bdad-a64a3d69cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does it make sense to look at progressions for the validation set?\n",
    "print(annotations_df[annotations_df['split']=='val']['Echo ID#'].value_counts())\n",
    "plot_pred_progression_study_level(227407, val_df[0], val_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fe1d0-c814-435e-8ac0-b514444628d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we hypothesize that for noisy labels in the dataset,\n",
    "# they are first wrong (pretty often), then eventually overfit to the correct answer\n",
    "# we create a \"surprisal\" metric for how different the GT is from the prediction progression\n",
    "def calculate_surprisal(epoch_0_dataframe, all_probs, count_after_t=0, count_before_t=-1):\n",
    "    assert count_after_t < len(all_probs)\n",
    "    if count_before_t != -1:\n",
    "        assert count_before_t > count_after_t and count_before_t < len(all_probs)\n",
    "    gt = np.array(epoch_0_dataframe['y'])\n",
    "    N = len(all_probs[0])\n",
    "\n",
    "    probs_after_t = all_probs[count_after_t:count_before_t]\n",
    "    probs_sum = np.array(probs_after_t).sum(axis=0)\n",
    "    surprisal = len(probs_after_t) - probs_sum[range(N), gt]\n",
    "    smoothed_label = probs_sum / len(probs_after_t)\n",
    "    return surprisal, smoothed_label\n",
    "\n",
    "train_surprisal, train_smoothed = calculate_surprisal(train_df[0], train_probs, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb92cb8d-6ad6-48ee-9af8-c16d484372b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10 surprisal values (and what their labels should be), visualize the training progression\n",
    "topk = 10\n",
    "top_k_indices = (-train_surprisal).argsort()[:topk]\n",
    "print(top_k_indices)\n",
    "for i in top_k_indices:\n",
    "    print(train_df[0]['y'][i])\n",
    "    print(train_smoothed[i])\n",
    "for i in top_k_indices:\n",
    "    plot_pred_progression(i, train_df[0], train_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8778c3f-5ed8-4d39-ae90-7156a8b55e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we process the entire dataset this way, how \"one-hot\" are the labels?\n",
    "N = len(train_smoothed)\n",
    "gt = np.array(train_df[0]['y'])\n",
    "smoothed_max = np.argmax(train_smoothed,axis=1)\n",
    "n_unclean_labels = np.sum(gt!=smoothed_max)\n",
    "print(n_unclean_labels)\n",
    "plt.hist([train_smoothed[range(N), gt],train_smoothed[range(N), smoothed_max]], bins=49)\n",
    "plt.legend(['original class', 'pseudo class'])\n",
    "plt.title('Label certainty distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d37e4d-5c6d-430c-8e62-d60a84886318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the smoothened labels back to the original dataframe\n",
    "# first create a corespondence between the rows of the original dataframe and the rows of train_df\n",
    "filenames = annotations_df['path']\n",
    "pseudolabels = []\n",
    "for j, entry in enumerate(list(filenames)):\n",
    "    full_filename = os.path.join(data_root, entry)\n",
    "    index_in_smoothed = train_df[0][train_df[0]['filename']==full_filename].index\n",
    "    if len(index_in_smoothed) <= 0: # validation or test set, fill with one-hot\n",
    "        pseudolabel = [0.0, 0.0, 0.0, 0.0]\n",
    "    else:\n",
    "        pseudolabel = train_smoothed[index_in_smoothed[0]]\n",
    "    pseudolabels.append(pseudolabel)\n",
    "\n",
    "pseudolabels = np.array(pseudolabels)\n",
    "pseudolabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e29e26-911c-4b22-898c-a4c1998ff492",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pseudolabel_csv = False\n",
    "if output_pseudolabel_csv:\n",
    "    for c in range(pseudolabels.shape[1]):\n",
    "        output_channel = f\"pseudolabel_{c}\"\n",
    "        annotations_df[output_channel] = pseudolabels[:, c]\n",
    "    annotations_df\n",
    "    # save the annotations to a new location\n",
    "    annotations_df.to_csv(os.path.join(csv_folder, 'annotations-all-with-pseudolabels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b1619-342d-4d7e-8ead-6dfba6660ee4",
   "metadata": {},
   "source": [
    "### Conformal prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548201d-8537-4350-9df3-ad6d034bfc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are setting up the validation set as the calibration set, to be more kosher, you should use an unseen subset of training\n",
    "# check the certainty distribution of validation set\n",
    "def certainty_hist(y, preds, epoch=-1):\n",
    "    # y is (N,) integer array of C classes\n",
    "    # preds is (N, C) array\n",
    "    pred_classes = np.argmax(preds,axis=1)\n",
    "    pred_confidence = np.max(preds,axis=1)\n",
    "    gt_confidence = preds[range(len(y)),y]\n",
    "    print(f\"Epoch {epoch}, F1 score = {f1_score(y, pred_classes, average='macro')}\")\n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    ax.hist([gt_confidence, pred_confidence], bins=49)\n",
    "    plt.legend(['GT', 'argmax'])\n",
    "    plt.title(f\"Epoch {i} label certainty distribution\")\n",
    "    plt.show()\n",
    "\n",
    "for i in range(len(val_df)):\n",
    "    y = val_df[i]['y']\n",
    "    preds = val_probs[i]\n",
    "    certainty_hist(y, preds, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6a887-fe14-4a96-9960-76a0df1e5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run conformal prediction on a select epoch to return the prediction sets\n",
    "def conformal_prediction(y, preds, desired_accuracy=0.9):\n",
    "    # y is (N,) integer array of C classes\n",
    "    # preds is (N, C) array\n",
    "    # desired_accuracy is equivalent to 1-alpha in CP literature\n",
    "    N = len(y)\n",
    "    pred_classes = np.argmax(preds,axis=1)\n",
    "    pred_confidence = np.max(preds,axis=1)\n",
    "    gt_confidence = preds[range(N),y]\n",
    "    # calculate the conformal score s as a general imprecise uncertainty measure (can be many ways, this is a simple way)\n",
    "    s = 1 - gt_confidence\n",
    "    # find the q level\n",
    "    q_level = np.quantile(s, desired_accuracy)\n",
    "    adj_q_level = np.ceil((N+1) * desired_accuracy)/N\n",
    "    # create sets based on the q-level\n",
    "    cutoff = 1 - adj_q_level\n",
    "    conformal_set = []\n",
    "    for i in range(N):\n",
    "        conformal_set.append(preds[i] >= cutoff)\n",
    "    return np.array(conformal_set)\n",
    "\n",
    "y = val_df[12]['y']\n",
    "preds = val_probs[12]\n",
    "cs = conformal_prediction(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f621ac48-5fe5-49ea-a82f-d1bb92088577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the coverage of conformal prediction\n",
    "def coverage_test(y, preds_cover):\n",
    "    # y is (N,) integer array of C classes\n",
    "    # preds_cover is (N, C) boolean array indicating the cover\n",
    "    correct = []\n",
    "    for i in range(len(y)):\n",
    "        if preds_cover[i, y[i]]:\n",
    "            correct.append(True)\n",
    "        else:\n",
    "            correct.append(False)\n",
    "    return np.array(correct)\n",
    "\n",
    "correctness = coverage_test(y, cs) \n",
    "print(np.sum(correctness)/len(y)) # should be around 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d19912a-1728-4c01-ac97-dc5b3579a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_conditional_coverage_test(y, preds_cover, num_classes=4):\n",
    "    # returns an accuracy for each class\n",
    "    class_cond_acc = np.zeros(num_classes)\n",
    "    for c in range(num_classes):\n",
    "        mask = y == c\n",
    "        preds_c = preds_cover[mask]\n",
    "        N_c = len(preds_c)\n",
    "        correct = 0\n",
    "        for i in range(N_c):\n",
    "            if preds_c[i,c]:\n",
    "                correct += 1\n",
    "        class_cond_acc[c] = correct/N_c\n",
    "    return class_cond_acc\n",
    "correctness = class_conditional_coverage_test(y, cs, 4) \n",
    "correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eaa033-e79c-40d9-b7ab-8ffba43c937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the cardinality of the conformal prediction\n",
    "def cp_cardinality(y, preds_cover, preds):\n",
    "    cardinality = np.sum(preds_cover, axis=1)\n",
    "    correct = coverage_test(y, preds_cover)\n",
    "    card_correct = cardinality[correct]\n",
    "    card_incorrect = cardinality[~correct]\n",
    "    \n",
    "    c_amount, c_bins = np.histogram(card_correct, bins=np.arange(5)+0.5)\n",
    "    n_amount, _ = np.histogram(card_incorrect, bins=np.arange(5)+0.5)\n",
    "    card_acc = c_amount / (c_amount + n_amount + 1e-9)\n",
    "    print(f\"Number of correct preds of size 1/2/3/4: {c_amount}\")\n",
    "    print(f\"Number of incorrect preds of size 1/2/3/4: {n_amount}\")\n",
    "    print(f\"Accuracy of preds of size 1/2/3/4: {card_acc}\")\n",
    "\n",
    "    top1_accs = []\n",
    "    pred_classes = np.argmax(preds, axis=1)\n",
    "    for i in np.arange(4)+1:\n",
    "        gt_w_card_i = y[cardinality==i]\n",
    "        pred_w_card_i = pred_classes[cardinality==i]\n",
    "        acc_w_card_i = np.sum(gt_w_card_i == pred_w_card_i) / (len(gt_w_card_i) + 1e-9)\n",
    "        top1_accs.append(acc_w_card_i)\n",
    "    print(f\"Top-1 acc of preds of size 1/2/3/4: {top1_accs}\")\n",
    "\n",
    "cp_cardinality(y, cs, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d7ffc-fe0c-4cba-ae0a-1fb0982aa26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run conformal prediction on each epoch\n",
    "f1s = []\n",
    "coverages = []\n",
    "for i in range(len(val_df)):\n",
    "    y = np.array(val_df[i]['y'])\n",
    "    preds = val_probs[i]\n",
    "    pred_classes = np.argmax(preds,axis=1)\n",
    "    print(f\"Epoch {i}\")\n",
    "    print(f\"Original classifier F1 = {f1_score(y, pred_classes, average='macro')}\")\n",
    "    f1s.append(f1_score(y, pred_classes, average='macro'))\n",
    "    cs = conformal_prediction(y, preds)\n",
    "    correctness = coverage_test(y, cs)\n",
    "    print(f\"Conformal coverage accuracy = {np.sum(correctness)/len(y)}\")\n",
    "    coverages.append(np.sum(correctness)/len(y))\n",
    "    correctness = class_conditional_coverage_test(y, cs, num_classes=4)\n",
    "    print(f\"Class-conditional coverage accuracy = {correctness}\")\n",
    "    cp_cardinality(y, cs, preds)\n",
    "    \n",
    "plt.plot(range(len(val_df)), f1s)\n",
    "plt.plot(range(len(val_df)), coverages)\n",
    "plt.plot(range(len(val_df)), np.full(shape=len(val_df), fill_value=0.9))\n",
    "plt.title('Label evolution: validation set top-1 F1 vs coverage over training')\n",
    "plt.legend(['top-1 F1 score', 'coverage acc.', 'theoretical coverage acc.'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd00a49-379e-45b2-9631-f32849449fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problems so far\n",
    "# 1. class-conditional coverage - can we have some CP algo with better guarantee for ea. class?\n",
    "# 1a. review RAPS algorithm\n",
    "# 2. cardinality relation w. difficulty - is it possible for low-cardinality sets to generally be more accurate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
